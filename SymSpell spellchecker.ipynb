{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898371\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# tokenize text using nltk lib\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "# combines all previous methods together\n",
    "def preprocess(text):\n",
    "    return tokenize(text.lower())\n",
    "\n",
    "\n",
    "def get_deletions(word, n=1):\n",
    "    dels = set()\n",
    "    for i in range(len(word)):\n",
    "        dels.add(word[:i] + word[i + 1:])\n",
    "    if n > 1:\n",
    "        tmp = set()\n",
    "        for w in dels:\n",
    "            tmp.update(get_deletions(w, n - 1))\n",
    "        tmp.update(dels)\n",
    "        return tmp\n",
    "    return dels\n",
    "\n",
    "\n",
    "def stem(word):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    return ps.stem(word)\n",
    "\n",
    "\n",
    "def get_reuters_tokens(path='reuters21578/'):\n",
    "    tokens = []\n",
    "    for f in glob.glob(path + \"reut2*.sgm\"):\n",
    "        reuter_stream = open(f, encoding=\"latin - 1\")\n",
    "        reuter_content = reuter_stream.read()\n",
    "        soup = BeautifulSoup(reuter_content, \"html.parser\")\n",
    "        articles = soup.find_all(\"reuters\")\n",
    "        for a in articles:\n",
    "            title = \"\" if not a.title else a.title.string\n",
    "            # id = a.get(\"newid\")\n",
    "            body = \"\" if not a.body else a.body.string\n",
    "            tmp = \"\\n\".join([title, body])\n",
    "            if len(tmp) < 1:\n",
    "                continue\n",
    "            tokens.extend(preprocess(tmp))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_big_tokens(path='big.txt'):\n",
    "    def words(text): return re.findall(r'\\w+', text.lower())\n",
    "    return list(words(open(path).read()))\n",
    "\n",
    "\n",
    "def build_index(tokens, del_depth=1):\n",
    "    \"\"\"\n",
    "    # principal function - builds an index of terms in all documents\n",
    "    # returns index: {word: {set_of_orig_words}}\n",
    "    :param path: path to directory with original reuters files\n",
    "    \"\"\"\n",
    "    index = defaultdict(set)\n",
    "    freq = Counter()\n",
    "    for token in tokens:\n",
    "        freq[token] += 1\n",
    "        for w in get_deletions(token, del_depth):\n",
    "            if w in index:\n",
    "                index[w].add(token)\n",
    "            else:\n",
    "                index[w] = {token}\n",
    "    return index, freq\n",
    "\n",
    "\n",
    "depth = 2\n",
    "index, freq = build_index(get_big_tokens(), depth)\n",
    "print(len(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance and correction (of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually this is Damerau Levenshtein \n",
    "# from: https://www.guyrutenberg.com/2008/12/15/damerau-levenshtein-distance-in-python/\n",
    "\"\"\"\n",
    "Compute the Damerau-Levenshtein distance between two given\n",
    "strings (s1 and s2)\n",
    "\"\"\"\n",
    "def distance(s1, s2):\n",
    "    xrange = range\n",
    "    d = {}\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    for i in xrange(-1,lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in xrange(-1,lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    " \n",
    "    for i in xrange(lenstr1):\n",
    "        for j in xrange(lenstr2):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1, # deletion\n",
    "                           d[(i,j-1)] + 1, # insertion\n",
    "                           d[(i-1,j-1)] + cost, # substitution\n",
    "                          )\n",
    "            if i and j and s1[i]==s2[j-1] and s1[i-1] == s2[j]:\n",
    "                d[(i,j)] = min (d[(i,j)], d[i-2,j-2] + cost) # transposition\n",
    " \n",
    "    return d[lenstr1-1,lenstr2-1]\n",
    "\n",
    "distance('poetry', 'peotry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poetry'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predecessors(words):\n",
    "    result = set()\n",
    "    for w in words:\n",
    "        if w in index:\n",
    "            result.update(index[w])\n",
    "    return result\n",
    "    \n",
    "\n",
    "# Lets make assumption that depth is 1 or 2 only\n",
    "def correction_ss(word, depth=2, threshold=0):\n",
    "    # Depth 0\n",
    "    if freq[word] > threshold:\n",
    "        return word\n",
    "    \n",
    "    # Depth 1\n",
    "    # 1. get variants:\n",
    "    candidates = {}\n",
    "    # up 1: ~ !!insert!!\n",
    "    d1 = index[word] # set\n",
    "    if depth == 2:\n",
    "        d2 = set(d1) # due to index[word] contains depth 1 and 2\n",
    "    # down 1:\n",
    "    dels1 = get_deletions(word, 1) # == !!deletions!!\n",
    "    d1.update(dels1)\n",
    "    d1.update(get_predecessors(dels1)) # ~ !!swap!! or !!change!!\n",
    "#     print(d1)\n",
    "    result = word\n",
    "    max_freq = freq[word] \n",
    "    if len(d1) > 0:\n",
    "        for w in d1:\n",
    "#             print(w, freq[w])\n",
    "            if freq[w] > max_freq and freq[w] > threshold and distance(word,w) == 1:\n",
    "                result = w\n",
    "                max_freq = freq[w]\n",
    "        if max_freq > threshold:\n",
    "            return result\n",
    "        \n",
    "#     print(\"go deeper (d2)\")\n",
    "    # Depth 2\n",
    "    if depth == 2:\n",
    "        # down 2:\n",
    "        for w in dels1:\n",
    "            d2.update(index[w]) # up dels1\n",
    "        dels2 = get_deletions(word,2).difference(dels1)\n",
    "        d2.update(dels2)\n",
    "        for w in dels2:\n",
    "            d2.update(index[w])\n",
    "#         print(d2)\n",
    "        \n",
    "        result = word\n",
    "        max_freq = freq[word] \n",
    "        for w in d2:\n",
    "            if freq[w] > max_freq and freq[w] > threshold and distance(word,w) <= 2:\n",
    "                if distance(word,w) == 1:\n",
    "                    print(\"wow!\")\n",
    "                result = w\n",
    "                max_freq = freq[w]\n",
    "        if max_freq > threshold:\n",
    "            return result\n",
    "    return word \n",
    "\n",
    "correction_ss('peotry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norvig's Solution\n",
    "from here: http://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction_norvig(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SymSpell test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_tests pass\n",
      "75% of 270 correct (6% unknown) at 1967 words per second \n",
      "68% of 400 correct (11% unknown) at 2814 words per second \n"
     ]
    }
   ],
   "source": [
    "correction = correction_ss\n",
    "\n",
    "def unit_tests():\n",
    "    assert correction('speling') == 'spelling'              # insert\n",
    "    assert correction('korrectud') == 'corrected'           # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    return 'unit_tests pass'\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
    "    import time\n",
    "    start = time.clock()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "    for right, wrong in tests:\n",
    "        w = correction(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in WORDS)\n",
    "            if verbose:\n",
    "                print('correction({}) => {} ({}); expected {} ({})'\n",
    "                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "    dt = time.clock() - start\n",
    "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "          .format(good / n, n, unknown / n, n / dt))\n",
    "    \n",
    "def Testset(lines):\n",
    "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "    return [(right, wrong)\n",
    "            for (right, wrongs) in (line.split(':') for line in lines)\n",
    "            for wrong in wrongs.split()]\n",
    "\n",
    "print(unit_tests())\n",
    "spelltest(Testset(open('spell-testset1.txt'))) # Development set\n",
    "spelltest(Testset(open('spell-testset2.txt'))) # Final test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Norvig's original solution's unit tests passing\n",
    "\n",
    "unit_tests pass\n",
    "75% of 270 correct (6% unknown) at 27 words per second \n",
    "68% of 400 correct (11% unknown) at 27 words per second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests(corrector):\n",
    "    def unit_tests():\n",
    "        assert corrector('speling') == 'spelling'              # insert\n",
    "        assert corrector('korrectud') == 'corrected'           # replace 2\n",
    "        assert corrector('bycycle') == 'bicycle'               # replace\n",
    "        assert corrector('inconvient') == 'inconvenient'       # insert 2\n",
    "        assert corrector('arrainged') == 'arranged'            # delete\n",
    "        assert corrector('peotry') =='poetry'                  # transpose\n",
    "        assert corrector('peotryy') =='poetry'                 # transpose + delete\n",
    "        assert corrector('word') == 'word'                     # known\n",
    "        assert corrector('quintessential') == 'quintessential' # unknown\n",
    "        return 'unit_tests pass'\n",
    "\n",
    "    def spelltest(tests, verbose=False):\n",
    "        \"Run corrector(wrong) on all (right, wrong) pairs; report results.\"\n",
    "        import time\n",
    "        start = time.clock()\n",
    "        good, unknown = 0, 0\n",
    "        n = len(tests)\n",
    "        for right, wrong in tests:\n",
    "            w = corrector(wrong)\n",
    "            good += (w == right)\n",
    "            if w != right:\n",
    "                unknown += (right not in WORDS)\n",
    "                if verbose:\n",
    "                    print('corrector({}) => {} ({}); expected {} ({})'\n",
    "                          .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "        dt = time.clock() - start\n",
    "        print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "              .format(good / n, n, unknown / n, n / dt))\n",
    "\n",
    "    def Testset(lines):\n",
    "        \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "        return [(right, wrong)\n",
    "                for (right, wrongs) in (line.split(':') for line in lines)\n",
    "                for wrong in wrongs.split()]\n",
    "\n",
    "    print(unit_tests())\n",
    "    spelltest(Testset(open('spell-testset1.txt'))) # Development set\n",
    "    spelltest(Testset(open('spell-testset2.txt'))) # Final test set\n",
    "\n",
    "    print('new tests')\n",
    "    spelltest(Testset(open('aspell.txt')))\n",
    "    spelltest(Testset(open('wikipedia.txt')))\n",
    "    spelltest(Testset(open('birkbeck.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_tests pass\n",
      "75% of 270 correct (6% unknown) at 426 words per second \n",
      "68% of 400 correct (11% unknown) at 1147 words per second \n",
      "new tests\n",
      "43% of 531 correct (23% unknown) at 947 words per second \n",
      "61% of 2455 correct (24% unknown) at 698 words per second \n",
      "31% of 36133 correct (11% unknown) at 491 words per second \n"
     ]
    }
   ],
   "source": [
    "tests(correction_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_tests pass\n",
      "75% of 270 correct (6% unknown) at 17 words per second \n",
      "68% of 400 correct (11% unknown) at 24 words per second \n",
      "new tests\n",
      "43% of 531 correct (23% unknown) at 12 words per second \n",
      "61% of 2455 correct (24% unknown) at 18 words per second \n",
      "31% of 36133 correct (11% unknown) at 14 words per second \n"
     ]
    }
   ],
   "source": [
    "tests(correction_norvig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
